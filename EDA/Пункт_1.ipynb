{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bcc7a1-5b0e-4a2f-ba6d-d195517ba2ae",
   "metadata": {},
   "source": [
    "# 1.Характеристика логов: количетво записей, структура, поля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "194ff693-ff07-41d4-81f9-4b11beb30cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распаковано: /Users/slvic/Applications/HDFS_v1_extracted\n",
      "Найдено:\n",
      "HDFS.log: /Users/slvic/Applications/HDFS_v1_extracted/HDFS.log\n",
      "Event_occurrence_matrix.csv: /Users/slvic/Applications/HDFS_v1_extracted/preprocessed/Event_occurrence_matrix.csv\n",
      "anomaly_label.csv: /Users/slvic/Applications/HDFS_v1_extracted/preprocessed/anomaly_label.csv\n",
      "\n",
      " Собран RAW DataFrame: (11175629, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level</th>\n",
       "      <th>component</th>\n",
       "      <th>message</th>\n",
       "      <th>BlockId</th>\n",
       "      <th>pid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-11-09 20:35:18</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-11-09 20:35:18</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-11-09 20:35:19</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-11-09 20:35:19</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-11-09 20:35:19</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp level                     component  \\\n",
       "0 2008-11-09 20:35:18  INFO      dfs.DataNode$DataXceiver   \n",
       "1 2008-11-09 20:35:18  INFO              dfs.FSNamesystem   \n",
       "2 2008-11-09 20:35:19  INFO      dfs.DataNode$DataXceiver   \n",
       "3 2008-11-09 20:35:19  INFO      dfs.DataNode$DataXceiver   \n",
       "4 2008-11-09 20:35:19  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             message  \\\n",
       "0  Receiving block blk_-1608999687919862906 src: ...   \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...   \n",
       "2  Receiving block blk_-1608999687919862906 src: ...   \n",
       "3  Receiving block blk_-1608999687919862906 src: ...   \n",
       "4  PacketResponder 1 for block blk_-1608999687919...   \n",
       "\n",
       "                    BlockId  pid  \n",
       "0  blk_-1608999687919862906  143  \n",
       "1  blk_-1608999687919862906   35  \n",
       "2  blk_-1608999687919862906  143  \n",
       "3  blk_-1608999687919862906  145  \n",
       "4  blk_-1608999687919862906  145  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Характеристика логов (raw)\n",
      "Строк:11,175,629\n",
      "Колонки: ['timestamp', 'level', 'component', 'message', 'BlockId', 'pid']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component</th>\n",
       "      <td>string[python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message</th>\n",
       "      <td>string[python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlockId</th>\n",
       "      <td>string[python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dtype\n",
       "timestamp  datetime64[ns]\n",
       "level              object\n",
       "component  string[python]\n",
       "message    string[python]\n",
       "BlockId    string[python]\n",
       "pid                object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Диапозон дат: 2008-11-09 20:35:18 -> 2008-11-11 11:16:28\n",
      "Уровни: {'INFO': 10812836, 'WARN': 362793}\n",
      "Компоненты (топ-10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dfs.FSNamesystem</th>\n",
       "      <td>3700245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.DataNode$PacketResponder</th>\n",
       "      <td>3413350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.DataNode$DataXceiver</th>\n",
       "      <td>2518678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.FSDataset</th>\n",
       "      <td>1407597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.DataBlockScanner</th>\n",
       "      <td>120046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.DataNode</th>\n",
       "      <td>7002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.DataNode$DataTransfer</th>\n",
       "      <td>6946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.DataNode$BlockReceiver</th>\n",
       "      <td>1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfs.PendingReplicationBlocks$PendingReplicationMonitor</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      count\n",
       "component                                                  \n",
       "dfs.FSNamesystem                                    3700245\n",
       "dfs.DataNode$PacketResponder                        3413350\n",
       "dfs.DataNode$DataXceiver                            2518678\n",
       "dfs.FSDataset                                       1407597\n",
       "dfs.DataBlockScanner                                 120046\n",
       "dfs.DataNode                                           7002\n",
       "dfs.DataNode$DataTransfer                              6946\n",
       "dfs.DataNode$BlockReceiver                             1718\n",
       "dfs.PendingReplicationBlocks$PendingReplication...       47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных BlockId (в сэмпле): 575061\n",
      "Примеры сообщений:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Receiving block blk_-1608999687919862906 src: ...\n",
       "1    BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...\n",
       "2    Receiving block blk_-1608999687919862906 src: ...\n",
       "3    Receiving block blk_-1608999687919862906 src: ...\n",
       "4    PacketResponder 1 for block blk_-1608999687919...\n",
       "Name: message, dtype: string"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, re, zipfile, glob\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Iterator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ZIP_CANDIDATES = [\n",
    "    \"HDFS_v1 (1).zip\", \"HDFS_v1.zip\",\n",
    "    \"/mnt/data/HDFS_v1 (1).zip\", \"/mnt/data/HDFS_v1.zip\"\n",
    "]\n",
    "EXTRACT_DIR = \"HDFS_v1_extracted\"\n",
    "RAW_LIMIT = None\n",
    "\n",
    "#распаковка\n",
    "def unzip_hdfs(zip_candidates, target_dir) -> str:\n",
    "    zip_path = next((z for z in zip_candidates if os.path.exists(z)), None)\n",
    "    if not zip_path and os.path.isdir(target_dir):\n",
    "        print(\"Используем распакованное:\", os.path.abspath(target_dir))\n",
    "        return os.path.abspath(target_dir)\n",
    "    if not zip_path:\n",
    "        raise FileNotFoundError(\"HDFS_v1.zip не найден\")\n",
    "    os.makedirs (target_dir, exist_ok =True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(target_dir)\n",
    "    print(\"Распаковано:\", os.path.abspath(target_dir))\n",
    "    return os.path.abspath(target_dir)\n",
    "\n",
    "root = unzip_hdfs(ZIP_CANDIDATES, EXTRACT_DIR)\n",
    "\n",
    "#поиск нужных файлов\n",
    "def find_one(root: str, name: str) -> Optional[str]:\n",
    "    hits = glob.glob(os.path.join(root, \"**\", name), recursive =True)\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "p_log  = find_one(root,\"HDFS.log\")\n",
    "p_feat = find_one(root, \"Event_occurrence_matrix.csv\")  \n",
    "p_lab  = find_one(root, \"anomaly_label.csv\")\n",
    "\n",
    "print(\"Найдено:\")\n",
    "print(\"HDFS.log:\", p_log or '-')\n",
    "print(\"Event_occurrence_matrix.csv:\", p_feat or '-')\n",
    "print(\"anomaly_label.csv:\", p_lab or \"-\")\n",
    "\n",
    "#паттерны для парсинга строк HDFS\n",
    "#паттерн: YYMMDD PID LEVEL COMPONENT: MESSAGE\n",
    "RE_MAIN = re.compile(\n",
    "    r\"\"\"^(?P<date>\\d{6})\\s(?P<time>\\d{6})\\s+\n",
    "        (?P<pid>\\d+)\\s+\n",
    "        (?P<level>[A-Z]+)\\s+\n",
    "        (?P<component>[^:]+):\\s?\n",
    "        (?P<message>.*)$\n",
    "    \"\"\", re.VERBOSE\n",
    ")\n",
    "\n",
    "#Фоллбек (без pid)\n",
    "RE_FALLBACK = re.compile(\n",
    "    r\"\"\"^(?P<date>\\d{6})\\s(?P<time>\\d{6})\\s+\n",
    "        (?P<level>[A-Z]+)\\s+\n",
    "        (?P<component>[^:]+):\\s?\n",
    "        (?P<message>.*)$\n",
    "    \"\"\", re.VERBOSE\n",
    ")\n",
    "# Паттерн для поиска BlockId в тексте сообщения (используется внутри parse_line)\n",
    "RE_BLOCK = re.compile(r\"\\bblk_[\\-\\d]+\")\n",
    "\n",
    "def parse_line(line: str) -> Optional[Dict]:\n",
    "    s = line.rstrip(\"\\n\")\n",
    "    m = RE_MAIN.match(s) or RE_FALLBACK.match(s)\n",
    "    if not m: \n",
    "        return None \n",
    "    g = m.groupdict()\n",
    "    try: \n",
    "        ts = datetime.strptime(g[\"date\"] + \" \" + g[\"time\"], \"%y%m%d %H%M%S\")\n",
    "    except Exception:\n",
    "        ts = None\n",
    "    msg = g.get(\"message\", \"\")\n",
    "    bid = None\n",
    "    m_blk = RE_BLOCK.search(msg)\n",
    "    if m_blk:\n",
    "        bid = m_blk.group(0)\n",
    "    return {\n",
    "    \"timestamp\": ts, \n",
    "    \"level\": g.get(\"level\"),\n",
    "    \"component\": (g.get(\"component\")or \"\").strip(),\n",
    "    \"message\": msg, \n",
    "    \"BlockId\": bid, \n",
    "    \"pid\": g.get(\"pid\")\n",
    "    }\n",
    "    \n",
    "def stream_parse(path: str, limit:Optional[int]) -> Iterator[Dict]:\n",
    "    with open(path, \"r\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            rec = parse_line(line)\n",
    "            if rec:\n",
    "                yield rec\n",
    "            if limit is not None and (i+1) >= limit:\n",
    "                break\n",
    "\n",
    "# Если найден HDFS.log — извлекаем из него реальные поля (timestamp, level, component, message, BlockId, pid)\n",
    "# и формируем структурированный DataFrame\n",
    "if p_log and os.path.exists(p_log):\n",
    "    df = pd.DataFrame(stream_parse(p_log, RAW_LIMIT),\n",
    "                      columns=[\"timestamp\",\"level\",\"component\",\"message\",\"BlockId\",\"pid\"])\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"Парсер не распознал ни одной строки. Пришли 3–5 строк из HDFS.log для подстройки шаблона.\")\n",
    "        \n",
    "    # Приводим уровень логов (level) к верхнему регистру и проверяем корректность значений (INFO/WARN/ERROR)\n",
    "    df[\"level\"] = df[\"level\"].astype(str).str.upper()\n",
    "    bad_levels = df.loc[~df[\"level\"].str.fullmatch(r\"[A-Z]+\", na=False), \"level\"].unique()\n",
    "    if len(bad_levels):\n",
    "        print(\"Подозрительные значения level:\", bad_levels[:10])\n",
    "\n",
    "    df[\"component\"] = df[\"component\"].astype(\"string\")\n",
    "    df[\"message\"] = df[\"message\"].astype(\"string\")\n",
    "    df[\"BlockId\"] = df[\"BlockId\"].astype(\"string\")\n",
    "    print(f\"\\n Собран RAW DataFrame: {df.shape}\")\n",
    "    display(df.head(5))\n",
    "    \n",
    "    # Характеристика логов (структура после парсинга)\n",
    "    print(\"\\n Характеристика логов (raw)\")\n",
    "    print(\"Строк:\" f\"{len(df):,}\")\n",
    "    print(\"Колонки:\", list(df.columns))\n",
    "    display(pd.DataFrame(df.dtypes, columns=[\"dtype\"]))\n",
    "    if df[\"timestamp\"].notna().any():\n",
    "        print(\"Диапозон дат:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "    if df[\"level\"].notna().any():\n",
    "        print(\"Уровни:\", df[\"level\"].value_counts().to_dict())\n",
    "    if df[\"component\"].notna().any():\n",
    "        print(\"Компоненты (топ-10):\")\n",
    "        display(df[\"component\"].value_counts().head(10).to_frame(\"count\"))\n",
    "    if df[\"BlockId\"].notna().any():\n",
    "        print(\"Уникальных BlockId (в сэмпле):\", df[\"BlockId\"].nunique())\n",
    "    print(\"Примеры сообщений:\")\n",
    "    display(df[\"message\"].head(5))\n",
    "\n",
    "# Если HDFS.log отсутствует — используем готовые предобработанные данные (features + labels)\n",
    "else:\n",
    "    assert p_feat and p_lab, \"Нет HDFS.log и не был найден Event_occurrence_matrix.csv/anomaly_label.csv\"\n",
    "    #читаем features; если есть колонка Label в feautures - исключим ее\n",
    "    feat_cols = pd.read_csv(p_feat, nrows=0).columns.tolist()\n",
    "    usecols_feat = [c for c in feat_cols if c.lower() != 'label']\n",
    "    df_feat = pd.read_csv(p_feat, usecols=usecols_feat)\n",
    "    df_lab  = pd.read_csv(p_lab)\n",
    "\n",
    "# Нормализация ключевого поля для корректного объединения данных\n",
    "    df_feat.rename(columns={\"block_id\":\"BlockId\",\"blockid\":\"BlockId\"}, inplace = True)\n",
    "    df_lab.rename(columns={\"block_id\":\"BlockId\", \"blockid\":\"BlockId\"}, inplace = True)\n",
    "\n",
    "    df = df_feat.merge(df_lab[[\"BlockId\", \"Label\"]], on = \"BlockId\", how = \"left\")\n",
    "#Нормализация меток\n",
    "    norm = {\"normal\":\"Success\", \"anomaly\":\"Fail\", \"success\":\"Success\",\"fail\":\"Fail\"}\n",
    "    df['Label'] = df['Label'].map(lambda x: norm.get(str(x).lower(), x) if pd.notna(x) else x)\n",
    "    df[\"y\"] = pd.Series(np.where(df['Label'].eq(\"Fail\"),1,0), dtype=\"Int8\").where(df[\"Label\"].notna(), other = pd.NA)\n",
    "\n",
    "# Преобразуем признаки E# в компактный тип uint16 для экономии памяти\n",
    "    e_cols = [c for c in df.columns if c.startswith(\"E\") and c[1:].isdigit()]\n",
    "    if e_cols:\n",
    "        df[e_cols] = df[e_cols].fillna(0).astype('uint16')\n",
    "    print(f\"\\n RAW логов нет. Собран FEATURES DataFrame: {df.shape}\")\n",
    "    display(df.head(5))\n",
    "\n",
    "#Характеристика таблицы признаков\n",
    "    print(\"\\n ХАРАКТЕРИСТИКА ПРИЗНАКОВ (features+labels)\")\n",
    "    print(\"Строк:\", f\"{len(df):,}\")\n",
    "    print(\"Колонок:\", df.shape[1])\n",
    "    print(\"Колонки (первые 20):\", df.columns.tolist()[:20], \"…\")\n",
    "    display(pd.DataFrame(df.dtypes, columns=[\"dtype\"]).head(20))\n",
    "    print(\"Пропуски (топ-15):\")\n",
    "    display(df.isna().sum().sort_values(ascending=False).head(15).to_frame(\"nulls\"))\n",
    "    print(\"Примеры строк:\")\n",
    "    display(df.head(5))\n",
    "\n",
    "df.to_parquet(\"hdfs_logs.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c9b68-259b-47b1-ac24-fa425d16a407",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
